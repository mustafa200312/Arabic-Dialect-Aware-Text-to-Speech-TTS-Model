{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c356235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('FINAL_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56dda096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>dialect</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>المجتمع مثلكم بتقول عيونك زايغه تطالع النسوان ...</td>\n",
       "      <td>Khaleeji</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>لكل فعل رده فعل المصرين بداو بالسب والشتم وسائ...</td>\n",
       "      <td>Khaleeji</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تحسين بضمير تري بنتك تقدر شي عكس حتنقهر عشانك ...</td>\n",
       "      <td>Khaleeji</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>مجانين لولوه الاكثر مصيبه الفئه الي تدافع تعز ...</td>\n",
       "      <td>Khaleeji</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>بمقوله غيره ارتاي الحق دربه فهو لحق اسبق يصرفه...</td>\n",
       "      <td>Khaleeji</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   dialect  word_count\n",
       "0  المجتمع مثلكم بتقول عيونك زايغه تطالع النسوان ...  Khaleeji          14\n",
       "1  لكل فعل رده فعل المصرين بداو بالسب والشتم وسائ...  Khaleeji          14\n",
       "2  تحسين بضمير تري بنتك تقدر شي عكس حتنقهر عشانك ...  Khaleeji          14\n",
       "3  مجانين لولوه الاكثر مصيبه الفئه الي تدافع تعز ...  Khaleeji          14\n",
       "4  بمقوله غيره ارتاي الحق دربه فهو لحق اسبق يصرفه...  Khaleeji          14"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65cb2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect = pd.get_dummies(df['dialect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8abe0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect = dialect.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab4c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.text.values.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c4d5284",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1500000\n",
    "tokenizer = Tokenizer(num_words=vocab_size, lower=False)\n",
    "tokenizer.fit_on_texts(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60aa3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, dialect, random_state=42, test_size=0.1, shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, random_state=42, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23e4aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tok = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_tok = pad_sequences(X_train_tok, maxlen=20)\n",
    "\n",
    "X_val_tok = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_tok = pad_sequences(X_val_tok, maxlen=20)\n",
    "\n",
    "X_test_tok = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_tok = pad_sequences(X_test_tok, maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e28bdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"NN/nn_best_model.h5\", monitor='loss',\n",
    "                            verbose=1, save_best_only=True, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51757708",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(Embedding(vocab_size, dialect.shape[1], input_shape=(20,)))\n",
    "model.add(SpatialDropout1D(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47adbb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6794/6800 [============================>.] - ETA: 0s - loss: 1.4549 - accuracy: 0.4256\n",
      "Epoch 1: loss improved from inf to 1.45473, saving model to NN\\nn_best_model.h5\n",
      "6800/6800 [==============================] - 62s 9ms/step - loss: 1.4547 - accuracy: 0.4257 - val_loss: 1.0570 - val_accuracy: 0.6197\n",
      "Epoch 2/10\n",
      "6799/6800 [============================>.] - ETA: 0s - loss: 1.0603 - accuracy: 0.6019\n",
      "Epoch 2: loss improved from 1.45473 to 1.06029, saving model to NN\\nn_best_model.h5\n",
      "6800/6800 [==============================] - 57s 8ms/step - loss: 1.0603 - accuracy: 0.6019 - val_loss: 0.9639 - val_accuracy: 0.6544\n",
      "Epoch 3/10\n",
      "6796/6800 [============================>.] - ETA: 0s - loss: 0.8958 - accuracy: 0.6692\n",
      "Epoch 3: loss improved from 1.06029 to 0.89580, saving model to NN\\nn_best_model.h5\n",
      "6800/6800 [==============================] - 59s 9ms/step - loss: 0.8958 - accuracy: 0.6693 - val_loss: 0.9302 - val_accuracy: 0.6670\n",
      "Epoch 4/10\n",
      "6799/6800 [============================>.] - ETA: 0s - loss: 0.7879 - accuracy: 0.7132\n",
      "Epoch 4: loss improved from 0.89580 to 0.78792, saving model to NN\\nn_best_model.h5\n",
      "6800/6800 [==============================] - 58s 9ms/step - loss: 0.7879 - accuracy: 0.7132 - val_loss: 0.9394 - val_accuracy: 0.6695\n",
      "Epoch 5/10\n",
      "6793/6800 [============================>.] - ETA: 0s - loss: 0.7020 - accuracy: 0.7450\n",
      "Epoch 5: loss improved from 0.78792 to 0.70178, saving model to NN\\nn_best_model.h5\n",
      "6800/6800 [==============================] - 52s 8ms/step - loss: 0.7018 - accuracy: 0.7451 - val_loss: 0.9573 - val_accuracy: 0.6692\n",
      "Epoch 6/10\n",
      "6796/6800 [============================>.] - ETA: 0s - loss: 0.6393 - accuracy: 0.7699\n",
      "Epoch 6: loss improved from 0.70178 to 0.63917, saving model to NN\\nn_best_model.h5\n",
      "6800/6800 [==============================] - 50s 7ms/step - loss: 0.6392 - accuracy: 0.7699 - val_loss: 0.9606 - val_accuracy: 0.6792\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_tok, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=15,\n",
    "    validation_data=(X_val_tok, y_val),\n",
    "    callbacks=[checkpoint, early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7db3de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6800/6800 [==============================] - ETA: 0s - loss: 1.4686 - accuracy: 0.4168\n",
      "Epoch 1: loss did not improve from 0.63917\n",
      "6800/6800 [==============================] - 53s 8ms/step - loss: 1.4686 - accuracy: 0.4168 - val_loss: 1.0629 - val_accuracy: 0.6063\n",
      "Epoch 2/10\n",
      "6797/6800 [============================>.] - ETA: 0s - loss: 1.0841 - accuracy: 0.5929\n",
      "Epoch 2: loss did not improve from 0.63917\n",
      "6800/6800 [==============================] - 58s 8ms/step - loss: 1.0841 - accuracy: 0.5928 - val_loss: 0.9929 - val_accuracy: 0.6405\n",
      "Epoch 3/10\n",
      "6799/6800 [============================>.] - ETA: 0s - loss: 0.9183 - accuracy: 0.6611\n",
      "Epoch 3: loss did not improve from 0.63917\n",
      "6800/6800 [==============================] - 62s 9ms/step - loss: 0.9182 - accuracy: 0.6612 - val_loss: 0.9676 - val_accuracy: 0.6553\n",
      "Epoch 4/10\n",
      "6795/6800 [============================>.] - ETA: 0s - loss: 0.8068 - accuracy: 0.7060\n",
      "Epoch 4: loss did not improve from 0.63917\n",
      "6800/6800 [==============================] - 61s 9ms/step - loss: 0.8068 - accuracy: 0.7060 - val_loss: 0.9950 - val_accuracy: 0.6517\n",
      "Epoch 5/10\n",
      "6799/6800 [============================>.] - ETA: 0s - loss: 0.7192 - accuracy: 0.7416\n",
      "Epoch 5: loss did not improve from 0.63917\n",
      "6800/6800 [==============================] - 59s 9ms/step - loss: 0.7192 - accuracy: 0.7416 - val_loss: 0.9910 - val_accuracy: 0.6644\n",
      "Epoch 6/10\n",
      "6796/6800 [============================>.] - ETA: 0s - loss: 0.6516 - accuracy: 0.7665\n",
      "Epoch 6: loss did not improve from 0.63917\n",
      "6800/6800 [==============================] - 47s 7ms/step - loss: 0.6517 - accuracy: 0.7664 - val_loss: 1.0179 - val_accuracy: 0.6623\n"
     ]
    }
   ],
   "source": [
    "model2 = models.Sequential()\n",
    "model2.add(Embedding(vocab_size, dialect.shape[1], input_shape=(20,)))\n",
    "model2.add(SpatialDropout1D(0.5))\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dense(7, activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "            optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history2 = model2.fit(\n",
    "    X_train_tok, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=15,\n",
    "    validation_data=(X_val_tok, y_val),\n",
    "    callbacks=[checkpoint, early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ca89c",
   "metadata": {},
   "source": [
    "# Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da0e0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"NN/nn_best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cabe841c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 - 1s - loss: 0.9923 - accuracy: 0.6692 - 601ms/epoch - 2ms/step\n",
      "\n",
      "Test accuracy: 0.6691812872886658\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test_tok, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
